# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cJqTuhUZmzmLGr2A6xWA-w2u9KnMyzAW

# Student ID: 2310348

Let's install all require libraries. For example, `transformers`
"""

!pip install transformers
!pip install -U -q PyDrive

"""Let's import all require libraries."""

import os
import re
import random
import pickle
import requests
import zipfile
import shutil
import argparse

import numpy as np
import pandas as pd
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf

from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, recall_score, precision_score, f1_score, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Importing necessary modules for Google Drive integration
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

"""**Let's put the student id as a variable, that you will use different places**"""

student_id = 2310348 # Note this is an interger and you need to input your id

"""Let's set `seed` for all libraries like `torch`, `numpy` etc as my student id"""

# set same seeds for all libraries
#numpy seed
np.random.seed(student_id)

"""# Common Codes

"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# Add your code to initialize GDrive and data and models paths
GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = './CE807-24-SP/Assignment/'
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

DATA_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'data', '8') # Make sure to replace 0 with last digit of your student Regitration number
train_file = os.path.join(DATA_PATH, 'train.csv')
print('Train file: ', train_file)

val_file = os.path.join(DATA_PATH, 'valid.csv')
print('Validation file: ', val_file)

test_file = os.path.join(DATA_PATH, 'test.csv')
print('Test file: ', test_file)


MODEL_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'model', str(student_id)) # Make sure to use your student Regitration number
MODEL_Gen_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Gen') # Model Generative directory
print('Model Generative directory: ', MODEL_Gen_DIRECTORY)

MODEL_Gen_File = MODEL_Gen_DIRECTORY + '.zip'


MODEL_Dis_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Dis') # Model Discriminative directory
print('Model Discriminative directory: ', MODEL_Dis_DIRECTORY)

MODEL_Dis_File = MODEL_Dis_DIRECTORY + '.zip'

"""# **Exploring the Dataset to Gain Insight**

Let's see train file
"""

train_df = pd.read_csv(train_file)
train_df.head()

# Explore dataset
print(train_df.shape)
print(train_df['toxicity'].value_counts())

# Visualizing the sentiments and there distribution
sns.countplot(x='toxicity', data=train_df)
plt.show()

"""The dataset exhibits class imbalance in the toxicity labels, with a significantly larger number of non-toxic comments (7568) compared to toxic ones (1131).

**Creating a word cloud visualization to represent the textual data.**

1. Input:
train_df: DataFrame containing comments with their toxicities labels as 0 and 1.

2. Processing:
Extracts non-toxic comments and toxic comments from the DataFrame based on the 'toxicity' column and joins the non-toxic comments and toxic comments separately into two strings.

3. Visualization:
Creates a matplotlib figure with two subplots side by side, generates word clouds for non-toxic comments and toxic comments using WordCloud and then Displays the word clouds in the respective subplots.

4. Output:
Displays the word clouds for non-toxic and toxic comments in separate subplots
"""

def visualize_wordclouds(train_df):
    """
    Visualize word clouds for positive (non-toxic) and negative (toxic) comments.

    Args:
    train_df (DataFrame): DataFrame containing the comments and their toxicities.

    Returns:
    None
    """
    non_toxic = ' '.join(train_df[train_df['toxicity'] == 0]['comment'])
    toxic = ' '.join(train_df[train_df['toxicity'] == 1]['comment'])

    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    wordcloud_pos = WordCloud(width=1600, height=600).generate(non_toxic)
    plt.imshow(wordcloud_pos, interpolation='bilinear')
    plt.title('Non Toxic Comments WordCloud')

    plt.subplot(1, 2, 2)
    wordcloud_neg = WordCloud(width=1600, height=600).generate(toxic)
    plt.imshow(wordcloud_neg, interpolation='bilinear')
    plt.title('Toxic Comments WordCloud')

    plt.show()

visualize_wordclouds(train_df)

"""The comments are observed to contain numerous unnecessary words and characters, necessitating the need for cleaning.

Let's observe the dataset
"""

# Calculate percentage of Class A (toxicity label 0) and Class B (toxicity label 1) in training dataset
class_a_train_percentage = (train_df['toxicity'] == 0).mean() * 100
class_b_train_percentage = (train_df['toxicity'] == 1).mean() * 100

print("Training Dataset:")
print(f"Class A (%): {class_a_train_percentage:.2f}")
print(f"Class B (%): {class_b_train_percentage:.2f}")

# Read validation dataset
valid_df = pd.read_csv(val_file)

# Read test dataset
test_df = pd.read_csv(test_file)

# Calculate percentage of Class A (toxicity label 0) and Class B (toxicity label 1) in validation dataset
class_a_valid_percentage = (valid_df['toxicity'] == 0).mean() * 100
class_b_valid_percentage = (valid_df['toxicity'] == 1).mean() * 100

print("Validation Dataset:")
print(f"Class A (%): {class_a_valid_percentage:.2f}")
print(f"Class B (%): {class_b_valid_percentage:.2f}")

# Get the total number of comments in the training dataset
total_comments_train = train_df.shape[0]

# Get the total number of comments in the validation dataset
total_comments_valid = valid_df.shape[0]

print("Training Dataset:")
print(f"Total Number of Comments: {total_comments_train}")

print("\nValidation Dataset:")
print(f"Total Number of Comments: {total_comments_valid}")

"""The above results indicate that the dataset is highly imbalanced

# **Cleaning and Preprocessing the data**

1. This code downloads necessary resources from NLTK such as tokenizers, stop words, and WordNet database.

2. The preprocess_text function preprocesses the input text by performing several text cleaning steps:

  1. Replacing 'NEWLINE_TOKEN' with a space.

  2. Removing unwanted prefixes and suffixes.

  3. Converting the text to lowercase.

  4. Removing URLs.

  5. Removing non-alphanumeric characters except single spaces.

  6. Tokenizing the text using NLTK's word tokenizer.

  7. Removing stop words from the tokenized words.

  8. Lemmatizing words to reduce them to their base or root form.

  9. Joining the lemmatized words back into a single string.

3. The preprocessed text is then returned.
"""

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text):
    # Replace 'NEWLINE_TOKEN' with space
    text = text.replace('NEWLINE_TOKEN', ' ')

    # Remove any unwanted prefixes and suffixes
    text = re.sub(r'SDATA_\d+ :', '', text)
    text = re.sub(r'EDATA_\d+', '', text)

    # Convert text to lowercase
    text = text.lower()

    # Remove URLs
    text = re.sub(r'http\S+', '', text)

    # Remove non-alphanumeric characters except single spaces
    text = re.sub(r'[^a-z0-9\s]', '', text)

    # Tokenize text
    words = nltk.word_tokenize(text)

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Lemmatize words to reduces words to their base or root form
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]

    # Join the words back into a single string
    text = ' '.join(words)

    return text

# Apply the preprocessing function to the first 'comment' column in train df
preprocessed_comment = preprocess_text(train_df.loc[0, 'comment'])

# Print the preprocessed comment
print(preprocessed_comment)

"""The initial observation suggests that the comment has been effectively cleaned through the preprocessing steps.

# **Common Modules**

We will utilize various performance metrics such as Accuracy, Recall (macro), Precision (macro), F1 (macro), and Confusion Matrix for evaluating performance. We'll print all the metrics and display the Confusion Matrix with appropriate X & Y axis labels.

1. This function computes various performance metrics such as accuracy, precision, recall, and F1 score for a given set of true and predicted labels.

2. It also displays a confusion matrix to visualize the model's performance in predicting different classes.

3. The performance metrics and the confusion matrix are printed and plotted for analysis and evaluation.
"""

def compute_performance(y_true, y_pred):
    """
    Prints different performance metrics like Accuracy, Recall (macro), Precision (macro), and F1 (macro).
    This also displays the Confusion Matrix with proper X & Y axis labels.
    Also, returns the F1 score.

    Args:
        y_true: numpy array or list
        y_pred: numpy array or list

    Returns:
        float: F1 score
    """

    # Calculate performance metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='macro')
    recall = recall_score(y_true, y_pred, average='macro')
    f1 = f1_score(y_true, y_pred, average='macro')

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')
    plt.show()

    # Print performance metrics
    print("Accuracy:", accuracy)
    print("Precision (macro):", precision)
    print("Recall (macro):", recall)
    print("F1 (macro):", f1)
    return f1

"""**Feature Engineering for Text Classification**

TF-IDF (Term Frequency-Inverse Document Frequency) involves assessing the significance of words in a document relative to their occurrence frequency across all documents.

Justification:

1. Dimensionality Reduction: TF-IDF helps in reducing the dimensionality of the feature space by focusing on the important words while downweighting the less important ones. This is crucial for managing the curse of dimensionality, especially in high-dimensional text data.

2. Importance of Words: TF-IDF assigns higher weights to words that are more informative within a specific document but less frequent across all documents. This helps in capturing the semantic meaning and context of the text data, as words that are rare in the entire corpus but frequent in a specific document are likely to be more discriminative.

3. Handling Stop Words: TF-IDF automatically handles common stop words by assigning them lower weights, as they tend to appear frequently across all documents and may not provide much discriminatory power.

4. Normalization: TF-IDF normalizes the term frequencies based on the inverse document frequency, which helps in addressing the issue of varying document lengths. This ensures that longer documents do not dominate the feature space.

5. Interpretability: TF-IDF generates feature vectors that are relatively interpretable, as each dimension represents the importance of a specific word within the document. This allows analysts and domain experts to understand and interpret the underlying patterns captured by the model.

6. Compatibility with Supervised Learning: TF-IDF is compatible with a wide range of supervised learning algorithms, making it suitable for tasks such as text classification, sentiment analysis, and document clustering.
"""

def transform_data(train_df, valid_df, test_df=None, model=None, max_features=5000):
    """
    Performs TF-IDF vectorization on the text data in train_df, valid_df, and test_df.

    Args:
        train_df (DataFrame): Training data containing 'cleaned_comment' and 'toxicity' columns.
        valid_df (DataFrame): Validation data containing 'preprocessed_comment' and 'toxicity' columns.
        test_df (DataFrame, optional): Test data containing 'preprocessed_comment' and 'out_label_model_Dis' or 'out_label_model_Gen' columns. Defaults to None.
        model (str, optional): Model type, either 'dis' or 'gen'. Defaults to None.
        max_features (int, optional): Maximum number of features for TF-IDF vectorization. Defaults to 5000.

    Returns:
        tuple: X_train, X_valid, X_test, y_train, y_valid, y_test
    """
    # Initialize TF-IDF vectorizer
    tfidf_vectorizer = TfidfVectorizer(max_features=max_features)

    # Fit and transform training data
    X_train = tfidf_vectorizer.fit_transform(train_df['comment'])
    y_train = train_df['toxicity']

    # Transform validation and test data using the same vectorizer
    X_valid = tfidf_vectorizer.transform(valid_df['comment'])
    if test_df is not None:
        X_test = tfidf_vectorizer.transform(test_df['comment'])
        y_test = test_df['out_label_model_Dis'] if model == "dis" else test_df['out_label_model_Gen']
    else:
        X_test, y_test = None, None

    y_valid = valid_df['toxicity']

    return X_train, X_valid, X_test, y_train, y_valid, y_test

def save_model(model, model_dir):
    """
    Saves the trained model to disk.

    Args:
        model: Trained model object.
        model_dir (str): Directory path where the model will be saved.

    Returns:
        str: File path of the saved model.
    """
    # Check if the model directory exists
    if not os.path.exists(model_dir):
        # Create the directory if it doesn't exist
        os.makedirs(model_dir)
        print(f"Directory '{model_dir}' created successfully.")
    else:
        print(f"Directory '{model_dir}' already exists.")

    # Define the file path for saving the model
    model_file = os.path.join(model_dir, 'model.sav')

    # Save the model to disk
    with open(model_file, 'wb') as f:
        pickle.dump(model, f)

    print('Saved model to:', model_file)

    return model_file

def load_model(model_file):
    """
    Loads a trained model from disk.

    Args:
        model_file (str): File path of the saved model.

    Returns:
        model: Loaded model object.
    """
    if model_file is None:
        raise ValueError("Model file path is None")

    # Check if the file exists
    if os.path.exists(model_file):
        # Load the model from disk
        with open(model_file, 'rb') as f:
            model = pickle.load(f)
        print('Loaded model from:', model_file)
        return model
    else:
        raise FileNotFoundError("Model file not found:", model_file)

"""# Let's download GDrive Link into a directory"""

def extract_file_id_from_url(url):
    """
    Extracts the file ID from a Google Drive or Google Docs URL.

    Args:
        url (str): URL of the file.

    Returns:
        str: File ID extracted from the URL.
    """
    file_id = None
    if 'drive.google.com' in url:
        file_id = url.split('/')[-2]
    elif 'https://docs.google.com' in url:
        file_id = url.split('/')[-1]

    return file_id

def download_file_from_drive(file_id, file_path):
    """
    Downloads a file from Google Drive using the file ID and saves it to the specified file path.

    Args:
        file_id (str): File ID of the file to download.
        file_path (str): Path where the downloaded file will be saved.
    """
    # Construct the download URL
    download_url = f"https://drive.google.com/uc?id={file_id}"

    # Download the file
    response = requests.get(download_url)
    if response.status_code == 200:
        with open(file_path, 'wb') as f:
            f.write(response.content)
        print("File downloaded successfully:", file_path)
    else:
        print("Failed to download the file.")

def download_zip_file_from_link(file_url, file_path):
    """
    Downloads a zip file from a Google Drive or Google Docs link and saves it to the specified file path.

    Args:
        file_url (str): URL of the zip file.
        file_path (str): Path where the downloaded zip file will be saved.
    """
    file_id = extract_file_id_from_url(file_url)
    if file_id:
        download_file_from_drive(file_id, file_path)
    else:
        print("Invalid Google Drive URL.")

"""# Zip and Unzip a GDrive File"""

def zip_directory(directory, zip_filename):
    """
    Zips a directory and its contents into a zip file.

    Args:
        directory (str): Path to the directory to be zipped.
        zip_filename (str): Name of the output zip file.
    """
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(directory):
            for file in files:
                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, '..')))
        print('Created a zip file', zip_filename)

def unzip_file(zip_filename, extract_dir):
    """
    Unzips a zip file into the specified extraction directory.

    Args:
        zip_filename (str): Path to the zip file to be extracted.
        extract_dir (str): Directory where the contents of the zip file will be extracted.
    """
    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
    print('Extracted a zip file to', extract_dir)

"""# Get Sharable link of your Zip file in Gdrive"""

def get_gdrive_link(file_path):
    """
    Generates a shareable link for a file stored in Google Drive.

    Args:
        file_path (str): Path to the file in Google Drive.

    Returns:
        str: Shareable link for the file.
    """
    # Authenticate and create PyDrive client
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)

    # Find the file in Google Drive
    file_name = file_path.split('/')[-1]
    file_list = drive.ListFile({'q': f"title='{file_name}'"}).GetList()

    # Get the file ID and generate the shareable link
    if file_list:
        file_id = file_list[0]['id']
        gdrive_link = f"https://drive.google.com/file/d/{file_id}/view?usp=sharing"
        return gdrive_link
    else:
        return "File not found in Google Drive"

def get_shareable_link(url):
    """
    Generates a shareable link for a file with the specified URL.

    Args:
        url (str): URL of the file.

    Returns:
        str: Shareable link for the file.
    """
    file_id = extract_file_id_from_url(url)

    # Authenticate and create PyDrive client
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)

    try:
        file_obj = drive.CreateFile({'id': file_id})
        file_obj.FetchMetadata()
        file_obj.InsertPermission({
            'type': 'anyone',
            'value': 'anyone',
            'role': 'reader'
        })

        # Get the shareable link
        return file_obj['alternateLink']
    except Exception as e:
        print("Error:", e)
        return None

"""# Method Generative Start

**train_Gen Function Breakdown:**

1. Loads and preprocesses training and validation datasets.

2. Constructs a pipeline with TF-IDF vectorization, SMOTE for class imbalance, and Multinomial Naive Bayes classifier.

3. Searches for the best hyperparameters using GridSearchCV with 5-fold cross-validation.

4. Evaluates the best model on the validation set and prints the F1 score.

5. Saves the best model and creates a shareable link for download.

6. Returns the shareable link of the saved model.
"""

def train_Gen(train_file, val_file, model_dir):
    """
    Takes train_file, val_file, and model_dir as input.
    Trains on the train_file datapoints and validates on the val_file datapoints.
    Prints different evaluation metrics and losses wherever necessary during training and validation.
    After training, saves the best model in the model_dir.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory
    """
    # Apply preprocessing
    train_df = pd.read_csv(train_file)
    valid_df = pd.read_csv(val_file)

    train_df['comment'] = train_df['comment'].apply(preprocess_text)
    valid_df['comment'] = valid_df['comment'].apply(preprocess_text)

    # Define pipeline
    pipeline = ImbPipeline([
        ('tfidf', TfidfVectorizer(max_features=5000)),
        ('smote', SMOTE(random_state=student_id)),
        ('nb', MultinomialNB())
    ])

    # Define the parameter grid
    param_grid = {
        'tfidf__max_features': [2000, 3000, 4000, 5000],
        'nb__alpha': [0.1, 0.5, 1.0]
    }

    # Setup GridSearchCV
    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='f1_macro', verbose=2, n_jobs=-1)

    # Train the model with GridSearchCV
    grid_search.fit(train_df['comment'], train_df['toxicity'])

    # Validate on the validation set
    nb_pred_valid = grid_search.predict(valid_df['comment'])

    # Save the best model
    best_model = grid_search.best_estimator_
    save_model(best_model, model_dir)

    # Compute and print performance metrics
    f1_score_valid = compute_performance(valid_df['toxicity'], nb_pred_valid)
    print("Validation F1 Score (best model):", f1_score_valid)

    # Now Zip Model to share it
    MODEL_Gen_File = "model_gen.zip"
    zip_directory(model_dir, MODEL_Gen_File)

    model_gdrive_link = get_gdrive_link(MODEL_Gen_File)

    print(model_gdrive_link)
    get_shareable_link(model_gdrive_link)

    return model_gdrive_link

"""**test_Gen Function Breakdown:**

1. Downloading Model: It starts by downloading the trained model from the provided Google Drive URL (model_gdrive_link). The model file is temporarily stored in a zip file (test_model_file).

2. Unzipping Model: After downloading, it unzips the model file to a temporary directory (test_model_path).

3. Loading Model: It loads the saved model from the unzipped directory.

4. Loading Test Data: It reads the test data from the provided test file (test_file).

5. Transforming Test Data: It transforms the test data using the same preprocessing steps applied during training and validation. This includes converting text data into TF-IDF vectors.

6. Making Predictions: It uses the loaded model to make predictions on the test data.

7. Saving Predictions: It saves the model predictions as a new column ('out_label_model_Gen') in the test DataFrame.

8. Saving Output: It saves the test DataFrame with model predictions back to the original test file.

9. Return: Finally, it returns the path of the test file with the model predictions.
"""

def test_Gen(test_file, MODEL_PATH, model_gdrive_link):
    """
    Takes test_file, MODEL_PATH, and model_gdrive_link as input.
    Loads the model and tests the examples in the test_file.
    Prints different evaluation metrics and saves the output in the output directory.

    Args:
        test_file: Test file name
        model_gdrive_link: Google Drive URL of the model
        MODEL_PATH: Directory of the model
    """

    print('\nStart by downloading model')

    # First, get model from the link
    # model_gdrive_link = get_gdrive_link(model_file)

    # These two are temporary directory and file
    test_model_file = MODEL_PATH+'/test.zip'
    test_model_path = MODEL_PATH+'/test/'

    # Now download and unzip the model file
    download_zip_file_from_link(model_gdrive_link, test_model_file)
    print('Model downloaded to', test_model_file)
    model = unzip_file(test_model_file, test_model_path)
    print('\nModel is downloaded to ',test_model_path)

    # Load the saved model
    model_path = MODEL_PATH+'/test/Model_Gen/model.sav'
    print(model_path)
    loaded_model = load_model(model_path)

    # Load test data
    test_df = pd.read_csv(test_file)

    X_train, X_valid, X_test, y_train, y_valid, y_test = transform_data(train_df, valid_df, test_df, "gen")

    # Make predictions using the loaded model
    predictions = loaded_model.predict(X_test)

    # Save the model output in the same test file
    # Note the name of the output column; this is for the discriminative model
    test_df['out_label_model_Gen'] = predictions

    # Save the model output in the same output file
    test_df.to_csv(test_file, index=False)
    print('\nOutput is saved in ', test_file)

    return test_file

"""## Method Generative End

# Method Discriminative Start

## Training Method Discriminative Code

**train_dis Function Breakdown:**

1. Reads and preprocesses training and validation datasets.

2. Constructs a pipeline with TF-IDF vectorization and Decision Tree classifier.

3. Defines parameter grid for hyperparameter tuning.

4. Performs grid search with 5-fold cross-validation to find the best model.

5. Prints the best hyperparameters found during grid search.

6. Validates the best model on the validation set and prints the F1 score.

7. Saves the best model in the specified directory.

8. Zips the saved model directory into a ZIP file named "model_dis.zip".

9. Generates a shareable link for the ZIP file using Google Drive.

10. Prints the shareable link of the ZIP file and a shareable link for direct download.

11. Returns the shareable link of the saved model.
"""

def train_dis(train_file, val_file, model_dir):
    """
    Takes train_file, val_file, and model_dir as input.
    Trains on the train_file datapoints and validates on the val_file datapoints.
    Prints different evaluation metrics and losses wherever necessary during training and validation.
    After finishing the training, it saves the best model in the model_dir.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory
        student_id: Student ID for setting random state

    Returns:
        model_gdrive_link: Google Drive link to the saved model
    """
    # Read data from files
    train_df = pd.read_csv(train_file)
    val_df = pd.read_csv(val_file)

    # Preprocess the data
    train_df['comment'] = train_df['comment'].apply(preprocess_text)
    val_df['comment'] = val_df['comment'].apply(preprocess_text)

    # Define pipeline with TfidfVectorizer, SMOTE, and DecisionTreeClassifier
    pipeline = ImbPipeline([
        ('tfidf', TfidfVectorizer(max_features=5000)),
        ('dt', DecisionTreeClassifier(random_state=student_id))
    ])

    # Define parameter grid for grid search
    param_grid = {
        'tfidf__max_features': [1000, 3000, 5000],
        'dt__max_depth': [None, 10, 20],
        'dt__min_samples_split': [2, 5, 10]
    }

    # Perform grid search
    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1')
    grid_search.fit(train_df['comment'], train_df['toxicity'])

    # Get best model and its parameters
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_

    # Print best parameters
    print("Best Parameters:", best_params)

    # Validate on the validation set
    dt_pred_valid = best_model.predict(val_df['comment'])

    # Compute performance on validation set
    f1_score_valid = compute_performance(val_df['toxicity'], dt_pred_valid)
    print("Validation F1 Score (best model):", f1_score_valid)

    # Save the best model
    save_model(best_model, model_dir)

   # Now Zip Model to share it
    MODEL_Dis_File = "model_dis.zip"
    zip_directory(model_dir, MODEL_Dis_File)

    model_gdrive_link = get_gdrive_link(MODEL_Dis_File)

    print(model_gdrive_link)
    get_shareable_link(model_gdrive_link)

    return model_gdrive_link

"""## Testing Method Discriminative Code

**test_dis Function Breakdown:**

1. The test_dis function takes three input arguments: test_file, MODEL_PATH, and model_gdrive_link.

2. It begins by printing a message indicating the start of downloading the model.

3. The function downloads the model from the provided Google Drive link and unzips it into a temporary directory.

4. It then constructs the path to the saved model file.

5. The saved model is loaded using the load_model function.

6. The function reads the test data from the CSV file and preprocesses it using the transform_data function.

7. It makes predictions on the test data using the loaded model.

8. The predicted labels are added to the test DataFrame under the column name out_label_model_Dis.

9. Finally, the function saves the updated test DataFrame to the specified output file and prints a message indicating the completion of the process.
"""

def test_dis(test_file, model_gdrive_link):
    """
    take test_file, model_file and output_dir as input.
    It loads model and test of the examples in the test_file.
    It prints different evaluation metrics, and saves the output in output directory

    ADD Other arguments, if needed

    Args:
        test_file: test file name
        model_file: model file name

    """
    print('\n Start by downloading model')
    # Frist Get model from the link
    # model_gdrive_link = get_gdrive_link(model_file)

    # These two are temporary directory and file
    test_model_file = MODEL_PATH+'/test.zip'
    test_model_path = MODEL_PATH+'/test/'

    # Now download and unzip the model file
    download_zip_file_from_link(model_gdrive_link,test_model_file)
    unzip_file(test_model_file, test_model_path)
    print('\n Model is downloaded to ',test_model_path)
    ##########################################################################
    #                     TODO: Implement this function                      #
    ##########################################################################
    # Replace "pass" statement with your code

    # Let's get test data
    test_df = pd.read_csv(test_file)
    print('\n Data is loaded from ', test_file)

    # Let's get the model file name & load it
    # Note you have to use same name a you used in the save

    test_model_file = os.path.join(test_model_path, 'Model_Dis', 'model.sav')

    model = load_model(test_model_file)
    # Let's do the prediction using test data
    y_pred= model.predict(test_df['comment'].apply(preprocess_text))

    # Now save the model output in the same test file
    # Note the name of output column, this is for the discriminative model
    test_df['out_label_model_Dis'] = y_pred

    # Now save the model output in the same output file
    test_df.to_csv(test_file, index=False)
    print('\n Output is save in ', test_file)

    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################
    return test_file

"""## Discriminative Method  End

# Other Method/model Start

## Other Models

**Logistic Regression Model**

1. Data Loading: The function reads the training and validation data from CSV files specified by train_file and val_file, respectively, using pd.read_csv().

2. Data Preprocessing: It preprocesses the text data in the 'comment' column of both the training and validation DataFrames using the preprocess_text() function.

3. Feature Engineering: The text data is transformed into numerical features using TF-IDF vectorization. It initializes a TfidfVectorizer with a maximum of 5000 features and applies it to the 'comment' column of the training data (X_train) and validation data (X_valid).Furthermore, the classes are balanced using SMOTE approach.

4. Model Training: It trains a logistic regression model (LogisticRegression) on the transformed training data (X_train) with corresponding labels (y_train). The model is configured to allow a maximum of 1000 iterations and uses a random seed specified by random_state.

5. Prediction: After training, the model predicts the labels for the validation data (X_valid) and stores the predictions in logistic_pred.

6. Evaluation: The function computes the F1 score on the validation set using the compute_performance() function, comparing the predicted labels (logistic_pred) with the true labels (y_valid). The computed F1 score is stored in f1_score_valid.

7. Output: Finally, the function returns a tuple containing the trained logistic regression model (logistic_model) and the F1 score on the validation set (f1_score_valid).
"""

from imblearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.metrics import f1_score
import pandas as pd

def logistic_regression(train_file, val_file):
    """
    Performs feature engineering, applies SMOTE, and trains a logistic regression model.

    Args:
        train_file (str): File path for training data.
        val_file (str): File path for validation data.

    Returns:
        tuple: A tuple containing the trained logistic regression model and the F1 score on the validation set.
    """
    # Read data from files
    train_df = pd.read_csv(train_file)
    valid_df = pd.read_csv(val_file)

    # Preprocess the data
    train_df['comment'] = train_df['comment'].apply(preprocess_text)
    valid_df['comment'] = valid_df['comment'].apply(preprocess_text)

    # Define pipeline
    pipeline = ImbPipeline([
        ('tfidf', TfidfVectorizer(max_features=5000)),
        ('smote', SMOTE(random_state=student_id)),
        ('logistic', LogisticRegression(max_iter=1000, random_state=student_id))
    ])

    # Fit pipeline
    pipeline.fit(train_df['comment'], train_df['toxicity'])

    # Predictions
    logistic_pred = pipeline.predict(valid_df['comment'])

    # Compute F1 score on the validation set using compute_performance function
    f1_score_valid = compute_performance(valid_df['toxicity'], logistic_pred)

    return pipeline, f1_score_valid

logistic_regression(train_file, val_file)

"""**Random Forest Classifier**

1. Data Loading: The function reads the training and validation data from CSV files specified by train_file and val_file, respectively, using pd.read_csv().

2. Data Preprocessing: It preprocesses the text data in the 'comment' column of both the training and validation DataFrames using the preprocess_text() function.

3. Feature Engineering: The text data is transformed into numerical features using TF-IDF vectorization. It initializes a TfidfVectorizer with a maximum of 5000 features and applies it to the 'comment' column of the training data (X_train) and validation data (X_valid). Furthermore, the classes are balanced using SMOTE approach.

4. Model Training: It trains a random forest classifier (RandomForestClassifier) on the transformed training data (X_train) with corresponding labels (y_train). The model is configured with 100 decision trees (n_estimators=100) and uses a random seed specified by random_state.

5. Prediction: After training, the model predicts the labels for the validation data (X_valid) and stores the predictions in rf_pred_valid.

6. Evaluation: The function computes the F1 score on the validation set using the compute_performance() function, comparing the predicted labels (rf_pred_valid) with the true labels (y_valid). The computed F1 score is returned as the output of the function.
"""

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
import pandas as pd

def random_forest(train_file, val_file):
    """
    Train RandomForestClassifier model on the training data and evaluate its performance on the validation set.

    Args:
        train_file (str): Path to the training data file.
        val_file (str): Path to the validation data file.

    Returns:
        float: F1 score on the validation set.
    """

    # Read data from files
    train_df = pd.read_csv(train_file)
    valid_df = pd.read_csv(val_file)

    # Preprocess the data
    train_df['comment'] = train_df['comment'].apply(preprocess_text)
    valid_df['comment'] = valid_df['comment'].apply(preprocess_text)

    # Define the pipeline
    pipeline = ImbPipeline([
        ('tfidf', TfidfVectorizer(max_features=5000)),
        ('smote', SMOTE(random_state=student_id)),
        ('rf', RandomForestClassifier(n_estimators=100, random_state=student_id))
    ])

    # Train the pipeline on the training data
    pipeline.fit(train_df['comment'], train_df['toxicity'])

    # Predictions on the validation set
    rf_pred_valid = pipeline.predict(valid_df['comment'])

    # Compute F1 score on the validation set using compute_performance function
    f1_score_valid = compute_performance(valid_df['toxicity'], rf_pred_valid)

    return f1_score_valid

random_forest(train_file, val_file)

"""**Gradient Boosting Classifier**

1. Loads and preprocesses training and validation datasets.

2. Constructs a pipeline with TF-IDF vectorization and Gradient Boosting Classifier.

3. Trains the pipeline on the training data.

4. Predicts toxicity labels on the validation set.

5. Computes the F1 score on the validation set.

6. Returns the F1 score as the output of the function.
"""

from sklearn.ensemble import GradientBoostingClassifier

def gradient_boosting(train_file, val_file):
    """
    Train Gradient Boosting Classifier model on the training data and evaluate its performance on the validation set.

    Args:
        train_file (str): Path to the training data file.
        val_file (str): Path to the validation data file.

    Returns:
        float: F1 score on the validation set.
    """

    # Read data from files
    train_df = pd.read_csv(train_file)
    valid_df = pd.read_csv(val_file)

    # Preprocess the data
    train_df['comment'] = train_df['comment'].apply(preprocess_text)
    valid_df['comment'] = valid_df['comment'].apply(preprocess_text)

    # Define the pipeline
    pipeline = ImbPipeline([
        ('tfidf', TfidfVectorizer(max_features=5000)),
        ('gb', GradientBoostingClassifier(random_state=student_id))
    ])

    # Train the pipeline on the training data
    pipeline.fit(train_df['comment'], train_df['toxicity'])

    # Predictions on the validation set
    gb_pred_valid = pipeline.predict(valid_df['comment'])

    # Compute F1 score on the validation set using compute_performance function
    f1_score_valid = compute_performance(valid_df['toxicity'], gb_pred_valid)

    return f1_score_valid

gradient_boosting(train_file, val_file)

"""**Fill Predictions to compared with GT**

1. Loads saved generative and discriminative models from specified paths.

2. Reads and preprocesses the validation dataset.

3. Performs TF-IDF vectorization on the validation data.

4. Makes predictions using the generative model on the preprocessed validation data.

5. Makes predictions using the discriminative model on the preprocessed validation data.

6. Fills in the predictions in the validation DataFrame.

7. Returns the DataFrame with filled predictions for both models.
"""

def fill_predictions(val_file):
    """
    Fills in the predictions for generative and discriminative models in the validation set.

    Args:
        validation_file (str): File path of the validation set CSV file.

    Returns:
        pandas.DataFrame: DataFrame with filled predictions for both models.
    """

    model_path_gen = MODEL_PATH+'/test/Model_Gen/model.sav'
    model_path_dis = MODEL_PATH+'/test/Model_Dis/model.sav'

    # Check if both models exist
    if not os.path.exists(model_path_gen) or not os.path.exists(model_path_dis):
        print("One or both models do not exist at the specified paths.")
        return

    # Load the saved models
    generative_model = load_model(model_path_gen)
    discriminative_model = load_model(model_path_dis)

    # Read validation set
    validation_df =  pd.read_csv(val_file)
    # Preprocess the data

   # TF-IDF Vectorization
    tfidf_vectorizer = TfidfVectorizer(max_features=5000)
    X_valid = tfidf_vectorizer.fit_transform(validation_df['comment'].apply(preprocess_text))
    y_valid = train_df['toxicity']

    # Predictions using generative model
    gen_predictions = generative_model.predict(X_valid)

    # Predictions using discriminative model
    dis_predictions = discriminative_model.predict(validation_df['comment'].apply(preprocess_text))

    # Fill in the predictions in the DataFrame
    validation_df['out_label_model_Gen'] = gen_predictions
    validation_df['out_label_model_Dis'] = dis_predictions

    return validation_df

fill_predictions(val_file)

# Define argparse-like function
def parse_arguments(option):
    parser = argparse.ArgumentParser(description='Process some integers.')
    parser.add_argument('--option', '-o',  type=str, default=option, help='Description of your option.')
    args = parser.parse_args(args=[])
    return args

# Function to perform some action based on selected option
def perform_action(option):
    print("Performing action with option:", option)

    if option == '0':
      print('\n Okay Exiting!!! ')

    elif option == '1':
      print('\n Training Generative Model')
      model_gdrive_link = train_Gen(train_file,val_file,MODEL_Gen_DIRECTORY)
      print('Make sure to pass model URL in Testing',model_gdrive_link)

    elif option == '2':
      print('\n\n Pass the URL Not Variable !!!')
      print('\n Testing Generative Model')

      model_gen_url = 'https://drive.google.com/file/d/10BjVeTLBTPFZGId-ljwgs3yVSz-ki6PE/view?usp=sharing'
      test_Gen(test_file,MODEL_PATH, model_gen_url)

    elif option == '3':
      print('\n Training Disciminative Model')
      model_gdrive_link = train_dis(train_file,val_file,MODEL_Dis_DIRECTORY)
      print('Make sure to pass model URL in Testing',model_gdrive_link)
      print('\n\n Pass the URL Not Variable !!!')

    elif option == '4':
      print('\n\n Pass the URL Not Variable !!!')
      print('\n Testing Disciminative Model')
      model_dis_url = 'https://drive.google.com/file/d/1-kbhr5ZsDSYfraBamE_ChCnmpzYcYx5I/view?usp=sharing'
      test_dis(test_file, model_dis_url)

    elif option == '5':
      print('\n\n Pass the URL Not Variable !!!')
      print('\n Training Logistic Regression Model')
      logistic_regression(train_file, val_file)

    elif option == '6':
      print('\n\n Pass the URL Not Variable !!!')
      print('\n Training Random Forest Classifier')
      random_forest(train_file, val_file)

    elif option == '7':
      print('\n\n Pass the URL Not Variable !!!')
      print('\n Training Gradient Boosting Classifier')
      gradient_boosting(train_file, val_file)

    else:
      print('Wrong Option Selected. \n\nPlease select Correct option')
      main()


def main():

    # Get option from user input
    user_option = input("0. To Exit Code\n"
                     "1. Train Model Generative\n"
                    "2. Test Model Generative\n"
                    "3. Train Model Discriminative\n"
                    "4. Test Model Discriminative\n"
                    "5. Evaluate Logistic Regression Model\n"
                    "6. Evaluate Random Forest Classifier\n"
                    "7. Evaluate Gradient Boosting Classifier\n"
                    "Enter your option: ")

    args = parse_arguments(user_option)
    option = args.option
    perform_action(option)

main()

"""##Other Method/model End"""